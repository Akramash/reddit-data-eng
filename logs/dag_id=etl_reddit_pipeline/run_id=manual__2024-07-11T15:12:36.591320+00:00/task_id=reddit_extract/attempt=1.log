[2024-07-11T15:12:37.133+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extract manual__2024-07-11T15:12:36.591320+00:00 [queued]>
[2024-07-11T15:12:37.137+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extract manual__2024-07-11T15:12:36.591320+00:00 [queued]>
[2024-07-11T15:12:37.137+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-07-11T15:12:37.142+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extract> on 2024-07-11 15:12:36.591320+00:00
[2024-07-11T15:12:37.144+0000] {standard_task_runner.py:57} INFO - Started process 54 to run task
[2024-07-11T15:12:37.146+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extract', 'manual__2024-07-11T15:12:36.591320+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpo1i1pv18']
[2024-07-11T15:12:37.146+0000] {standard_task_runner.py:85} INFO - Job 14: Subtask reddit_extract
[2024-07-11T15:12:37.167+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extract manual__2024-07-11T15:12:36.591320+00:00 [running]> on host d7a6ef074cc8
[2024-07-11T15:12:37.196+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Akram Ibrahim' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extract' AIRFLOW_CTX_EXECUTION_DATE='2024-07-11T15:12:36.591320+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-11T15:12:36.591320+00:00'
[2024-07-11T15:12:37.198+0000] {logging_mixin.py:151} INFO - Connected to Reddit instance
[2024-07-11T15:12:37.885+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'We are currently running spark sql jobs every 15 mins. We grab about 10 GB of data during peak which has 100 columns then join it to about 25 other tables to enrich it and produce an output of approx 200 columns. A series of giant SQL batch jobs seems inefficient and slow. Any other ideas? Thanks.\n', 'author_fullname': 't2_szg2zfxvl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you use for realish time ETL?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0bt8z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 50, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 50, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1720670383.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720658189.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We are currently running spark sql jobs every 15 mins. We grab about 10 GB of data during peak which has 100 columns then join it to about 25 other tables to enrich it and produce an output of approx 200 columns. A series of giant SQL batch jobs seems inefficient and slow. Any other ideas? Thanks.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0bt8z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Trick-Interaction396'), 'discussion_type': None, 'num_comments': 50, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0bt8z/what_do_you_use_for_realish_time_etl/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0bt8z/what_do_you_use_for_realish_time_etl/', 'subreddit_subscribers': 195974, 'created_utc': 1720658189.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.886+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In my previous role at a startup I was hired as a data scientist to build various algorithms and analytical products that would be surfaced to users-- except for the huge problem of the data infrastructure and quality being awful. Through getting some key projects in production and collaborating with engineering, I quickly became the most technical on my team and was exposed to the entire data lifecycle.\n\nThis exposure made me become obsessed with data infrastructure to improve the lives of my data science colleagues and kicked off my journey towards data engineering. By the time I left the startup I became a Senior Data Engineer, migrated us to a data lakehouse architecture, and built data pipelines that connected various data silos together (eg combining Salesforce, product usages, and customer success data for business insights). I ended up loving data engineering way more than data science.\n\nIf I were transitioning from data science to data engineering again, this is how I would approach learning in the early stages:\n\n1. My first step would be to read Joe Reis and Matthew Housley\'s book "Fundamentals of Data Engineering" as it will give the best overview of the space and provide scaffolding and reference for later learning.\n\n2. Assuming you are already in a data role, start digging further into how data reaches the analytical database you are already working in. For example, if you are utilizing a data lakehouse architecture, go find the "bronze layer" and understand a) how it\'s ingested into the data lake, and b) how it\'s transformed into a "gold layer" downstream.\n\n3. Dive further into the "bronze layer" ingestion stage and specifically trace the lineage back to the data source from which it was replicated. There will be multiple sources, but ideally the one you choose is not a third-party source, and it points back to a transactional database used by upstream engineers. Throughout these steps in the lineage, make notes of the various transformations and tools leveraged throughout the process (e.g. orchestrators like Airflow).\n\n4. Once you trace the lineage to how data is ingested into the upstream transactional database, read the code that creates the event logs for data or the data ingestions into the transactional database. If possible, find people on the upstream engineering team who are open to walking you through their workflows for these steps (this helped me immensely in my own journey).\n\n5. Now that you have a high-level overview of the entire data lifecycle for a particular use case, pay special attention to how the data is structured differently in transactional and analytical databases. This will be the underpinning of more in-depth learning around data modeling, but that\'s a huge topic that will take time to feel competent in, so just focus on being exposed to the situations.\n\n6. With this real-world exposure, it\'s a great time to return to the "Fundamentals of Data Engineering" book and read all the chapter summaries to see how your knowledge has become much more nuanced.\n\n7. Your high-level understanding will enable you to independently build a data engineering project. There are many guides; just choose one. Focus on a project that requires ingesting real-world data (e.g., public API or S3 bucket), putting it into a local database like Postgres, adding some orchestration to automate it via Airflow (or whatever tool you like), and engineering best practices of logging and repo structure (look it up while coding).\n\n8. After this project you will not only have a real-world understanding from your earlier explorations but also from coding a project yourself. At this stage, I think it makes sense to move into more advanced reading such as Martin Kleppmann\'s "Designing Data-Intensive Applications" or the various data modeling books (e.g. Kimball or Inmon).\n\nNow will this make you ready to become a data engineer? Absolutely not, but this will give you a strong foundation to move in that direction!\n\nFor people so have made similar transitions from other roles, what other advice would you add?', 'author_fullname': 't2_v7fvlqc8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Transitioning From Data Science to Data Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0b3l3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 49, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 49, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720656171.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In my previous role at a startup I was hired as a data scientist to build various algorithms and analytical products that would be surfaced to users-- except for the huge problem of the data infrastructure and quality being awful. Through getting some key projects in production and collaborating with engineering, I quickly became the most technical on my team and was exposed to the entire data lifecycle.</p>\n\n<p>This exposure made me become obsessed with data infrastructure to improve the lives of my data science colleagues and kicked off my journey towards data engineering. By the time I left the startup I became a Senior Data Engineer, migrated us to a data lakehouse architecture, and built data pipelines that connected various data silos together (eg combining Salesforce, product usages, and customer success data for business insights). I ended up loving data engineering way more than data science.</p>\n\n<p>If I were transitioning from data science to data engineering again, this is how I would approach learning in the early stages:</p>\n\n<ol>\n<li><p>My first step would be to read Joe Reis and Matthew Housley&#39;s book &quot;Fundamentals of Data Engineering&quot; as it will give the best overview of the space and provide scaffolding and reference for later learning.</p></li>\n<li><p>Assuming you are already in a data role, start digging further into how data reaches the analytical database you are already working in. For example, if you are utilizing a data lakehouse architecture, go find the &quot;bronze layer&quot; and understand a) how it&#39;s ingested into the data lake, and b) how it&#39;s transformed into a &quot;gold layer&quot; downstream.</p></li>\n<li><p>Dive further into the &quot;bronze layer&quot; ingestion stage and specifically trace the lineage back to the data source from which it was replicated. There will be multiple sources, but ideally the one you choose is not a third-party source, and it points back to a transactional database used by upstream engineers. Throughout these steps in the lineage, make notes of the various transformations and tools leveraged throughout the process (e.g. orchestrators like Airflow).</p></li>\n<li><p>Once you trace the lineage to how data is ingested into the upstream transactional database, read the code that creates the event logs for data or the data ingestions into the transactional database. If possible, find people on the upstream engineering team who are open to walking you through their workflows for these steps (this helped me immensely in my own journey).</p></li>\n<li><p>Now that you have a high-level overview of the entire data lifecycle for a particular use case, pay special attention to how the data is structured differently in transactional and analytical databases. This will be the underpinning of more in-depth learning around data modeling, but that&#39;s a huge topic that will take time to feel competent in, so just focus on being exposed to the situations.</p></li>\n<li><p>With this real-world exposure, it&#39;s a great time to return to the &quot;Fundamentals of Data Engineering&quot; book and read all the chapter summaries to see how your knowledge has become much more nuanced.</p></li>\n<li><p>Your high-level understanding will enable you to independently build a data engineering project. There are many guides; just choose one. Focus on a project that requires ingesting real-world data (e.g., public API or S3 bucket), putting it into a local database like Postgres, adding some orchestration to automate it via Airflow (or whatever tool you like), and engineering best practices of logging and repo structure (look it up while coding).</p></li>\n<li><p>After this project you will not only have a real-world understanding from your earlier explorations but also from coding a project yourself. At this stage, I think it makes sense to move into more advanced reading such as Martin Kleppmann&#39;s &quot;Designing Data-Intensive Applications&quot; or the various data modeling books (e.g. Kimball or Inmon).</p></li>\n</ol>\n\n<p>Now will this make you ready to become a data engineer? Absolutely not, but this will give you a strong foundation to move in that direction!</p>\n\n<p>For people so have made similar transitions from other roles, what other advice would you add?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0b3l3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='on_the_mark_data'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0b3l3/transitioning_from_data_science_to_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0b3l3/transitioning_from_data_science_to_data/', 'subreddit_subscribers': 195974, 'created_utc': 1720656171.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.887+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "\nI was laid off from a company that contacted me yesterday to rehire me. They offered a senior data analyst position with a starting salary of $75K, which is insultingly low for the area(Atlanta, Georgia). They want 5 years of SQL experience, require an SQL test, and whiteboard exercises. The pay caps at $80K with only 10 PTO days and four paid holidays. I was shocked, especially since it's a Fortune 100 company. 75K horrendous, that's only 20K more than what I started with 5 years ago with a computer science degree out of a really well-known state university... The audacity of these people is unreal. Like, how do you say such a thing with a straight face?\n\n\nThe recruiter went off on some tangent about how people join their company for the good benefits, company culture, stable long-term employment and retire... Well, I was one of the top performers in my department and I got laid off without any notice. Like, how can they seriously say that? \n\n\nTagged as meme because of how meme like this is. ", 'author_fullname': 't2_hdeet8zsc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'You ever get offered an incredibly low salary? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e0pbaj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Meme', 'can_mod_post': False, 'score': 29, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720705521.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was laid off from a company that contacted me yesterday to rehire me. They offered a senior data analyst position with a starting salary of $75K, which is insultingly low for the area(Atlanta, Georgia). They want 5 years of SQL experience, require an SQL test, and whiteboard exercises. The pay caps at $80K with only 10 PTO days and four paid holidays. I was shocked, especially since it&#39;s a Fortune 100 company. 75K horrendous, that&#39;s only 20K more than what I started with 5 years ago with a computer science degree out of a really well-known state university... The audacity of these people is unreal. Like, how do you say such a thing with a straight face?</p>\n\n<p>The recruiter went off on some tangent about how people join their company for the good benefits, company culture, stable long-term employment and retire... Well, I was one of the top performers in my department and I got laid off without any notice. Like, how can they seriously say that? </p>\n\n<p>Tagged as meme because of how meme like this is. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff66ac', 'id': '1e0pbaj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='databro92'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0pbaj/you_ever_get_offered_an_incredibly_low_salary/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0pbaj/you_ever_get_offered_an_incredibly_low_salary/', 'subreddit_subscribers': 195974, 'created_utc': 1720705521.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.887+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Anyone else got a slightly embarrassing GitHub name? I’ve recently been thinking of adding my profile to job applications but the name is a bit stupid. Nothing offensive, just a stupid username I picked when I wasn’t thinking about using it for showcasing work. Anyone else got a name they regret using and does anyone include it in their job applications?', 'author_fullname': 't2_scnmi5ys', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone else got a personal GitHub name they regret?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0kvc9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 19, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 19, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720690594.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Anyone else got a slightly embarrassing GitHub name? I’ve recently been thinking of adding my profile to job applications but the name is a bit stupid. Nothing offensive, just a stupid username I picked when I wasn’t thinking about using it for showcasing work. Anyone else got a name they regret using and does anyone include it in their job applications?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0kvc9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Pleasant-Aardvark258'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0kvc9/anyone_else_got_a_personal_github_name_they_regret/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0kvc9/anyone_else_got_a_personal_github_name_they_regret/', 'subreddit_subscribers': 195974, 'created_utc': 1720690594.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.888+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "It's my situation, I lost my main job as a data analyst... I have 6 years of experience as an analyst and working with data is my vocation.\n\nLucky I have someone building a consulting firm and bringing me new projects. Last week I finished my first DE project that consisted in adapting SASGUIDE pipelines to Tableau prep environment and maybe I might take a new one that involves pentaho...But is still fresh and the pay is not the best. I might still need to look for a moderate 9-5 job while taking those projects.\n\nI want to become a more robust data professional, and for that, I have been following a very interesting training course track, but most of the time, it involves tool demonstrations and specific simulations of what is possible to do with them. So far, I have learned the concepts and fundamentals, and the basics of PySpark, Databricks, Hadoop/Hive, Athena/Glue, dbt-Redshift, and Airbyte. It is enough to understand where they take place rather than build a full solution end-to-end.\n\nI want to take advantage of this free time to apply this knowledge at zero cost, any idea of what to do that has a maximum cost-effort-benefit ratio?\n\nMy idea is not to worry about tool costs or data volume, so I want to run everything locally because I almost never turn off my laptop.\n\nWhat I have in mind is to refine some data projects that involve Python and apply OOP and documentation before publishing on GitHub:\n\n1. My personal finance spreadsheet from Google Sheets: I need to run my Python code to feed a table in the database, and there are more spreadsheets that I would like to consume data from. As I don't have tableau licenses anymore I might apply another viz tool.\n2. My mom's home solar energy system doesn't offer an API, so I built a web scraping script that made a request to an AJAX and returned a JSON of daily data and loaded into Postgres. Unfortunately, this flow broke, and I haven't had time to fix it. I would like to make it more robust so that daily requests are recursive, extracting data from missing days automatically (for example, when the computer is off and I miss a day or two), and instead of using the Windows Task Scheduler/ Linux cron to run the script, I would like to use Airbyte because it seems to work well for these things. What do you think?\n3. The stock market website allows extracting reports of all investments I've made in my life, I could wrangle it and load into my *** db too;\n\nMaybe insert dbt models when regarding data transformation in them?\n\nIs this a good plan or do you have any recomendation? \n\nthanks in advance", 'author_fullname': 't2_k5cftaq74', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'You need to reposition yourself in the job market - you have 1 month to learn new skills and do hands-on projects, what would you do?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e03997', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720636491.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>It&#39;s my situation, I lost my main job as a data analyst... I have 6 years of experience as an analyst and working with data is my vocation.</p>\n\n<p>Lucky I have someone building a consulting firm and bringing me new projects. Last week I finished my first DE project that consisted in adapting SASGUIDE pipelines to Tableau prep environment and maybe I might take a new one that involves pentaho...But is still fresh and the pay is not the best. I might still need to look for a moderate 9-5 job while taking those projects.</p>\n\n<p>I want to become a more robust data professional, and for that, I have been following a very interesting training course track, but most of the time, it involves tool demonstrations and specific simulations of what is possible to do with them. So far, I have learned the concepts and fundamentals, and the basics of PySpark, Databricks, Hadoop/Hive, Athena/Glue, dbt-Redshift, and Airbyte. It is enough to understand where they take place rather than build a full solution end-to-end.</p>\n\n<p>I want to take advantage of this free time to apply this knowledge at zero cost, any idea of what to do that has a maximum cost-effort-benefit ratio?</p>\n\n<p>My idea is not to worry about tool costs or data volume, so I want to run everything locally because I almost never turn off my laptop.</p>\n\n<p>What I have in mind is to refine some data projects that involve Python and apply OOP and documentation before publishing on GitHub:</p>\n\n<ol>\n<li>My personal finance spreadsheet from Google Sheets: I need to run my Python code to feed a table in the database, and there are more spreadsheets that I would like to consume data from. As I don&#39;t have tableau licenses anymore I might apply another viz tool.</li>\n<li>My mom&#39;s home solar energy system doesn&#39;t offer an API, so I built a web scraping script that made a request to an AJAX and returned a JSON of daily data and loaded into Postgres. Unfortunately, this flow broke, and I haven&#39;t had time to fix it. I would like to make it more robust so that daily requests are recursive, extracting data from missing days automatically (for example, when the computer is off and I miss a day or two), and instead of using the Windows Task Scheduler/ Linux cron to run the script, I would like to use Airbyte because it seems to work well for these things. What do you think?</li>\n<li>The stock market website allows extracting reports of all investments I&#39;ve made in my life, I could wrangle it and load into my *** db too;</li>\n</ol>\n\n<p>Maybe insert dbt models when regarding data transformation in them?</p>\n\n<p>Is this a good plan or do you have any recomendation? </p>\n\n<p>thanks in advance</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e03997', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='life_punches'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e03997/you_need_to_reposition_yourself_in_the_job_market/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e03997/you_need_to_reposition_yourself_in_the_job_market/', 'subreddit_subscribers': 195974, 'created_utc': 1720636491.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.888+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey there,\n\nMy workplace is giving me the freedom to explore new data migration strategies/tools.\n\nWe’re currently using CDC with SSIS to extract database tables from one SQL Server to our DWH SQL server.\n\nSSIS has worked, but it’s clunky and sometimes not easy to troubleshoot. \n\nI’m trying to look into other alternatives and have heard python could be useful. Python is new to me and I’ve only ever seen it used for quick data analysis.\n\nLooking for ideas and resources \n\nThanks \n\n', 'author_fullname': 't2_a4qeifs0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'SSIS CDC Alternative', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e04wuj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1720645102.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720640576.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey there,</p>\n\n<p>My workplace is giving me the freedom to explore new data migration strategies/tools.</p>\n\n<p>We’re currently using CDC with SSIS to extract database tables from one SQL Server to our DWH SQL server.</p>\n\n<p>SSIS has worked, but it’s clunky and sometimes not easy to troubleshoot. </p>\n\n<p>I’m trying to look into other alternatives and have heard python could be useful. Python is new to me and I’ve only ever seen it used for quick data analysis.</p>\n\n<p>Looking for ideas and resources </p>\n\n<p>Thanks </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e04wuj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lurch1400'), 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e04wuj/ssis_cdc_alternative/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e04wuj/ssis_cdc_alternative/', 'subreddit_subscribers': 195974, 'created_utc': 1720640576.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.889+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey I'm new to the field of data engineering(fresh mechanical graduate, no C.S background). Learned the basics of python, numpy, pandas and now my supervisor wants me to learn about fastAPI. Could anyone guide me on where I should get started? I've trying a few youtube tutorials but am having a hard time grasping the examples. I think I'm missing something in sense of theoretical knowledge. Could anyone guide me on the topics I should work on first or any yt recommendations.", 'author_fullname': 't2_ivlmwlk2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I want to learn FastAPI', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e04m8o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720639840.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey I&#39;m new to the field of data engineering(fresh mechanical graduate, no C.S background). Learned the basics of python, numpy, pandas and now my supervisor wants me to learn about fastAPI. Could anyone guide me on where I should get started? I&#39;ve trying a few youtube tutorials but am having a hard time grasping the examples. I think I&#39;m missing something in sense of theoretical knowledge. Could anyone guide me on the topics I should work on first or any yt recommendations.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e04m8o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Cloud_Lionhart'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e04m8o/i_want_to_learn_fastapi/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e04m8o/i_want_to_learn_fastapi/', 'subreddit_subscribers': 195974, 'created_utc': 1720639840.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.889+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Not sure if this is the best place to ask, but I'm more of a data scientist/engineer than a fullstack developer, but maybe you guys can help.\n\nI have a task to create a rather basic GUI application which should be able to run on a set schedule defined from the GUI, e.g. every 30 min or every hour between 8 am and 8 pm or smth. The user should be able to change the configuration and the job should react accordingly. The Jobs would be fetching data from APIs, hopefully this makes it at least DE adjacent :).\n\nHow would you approach this? Any references or best practices would be much appreciated.\n\nIn principle I could code inside the application a loop that is checking if the condition is met and initiate the API calls.\n\nI'm also wondering if this would be an appropriate use of e.g. airflow (funny enough never used it in a production setting) or something like RabbitMQ? Or is it overkill/over-engineering?\n\nI'm comfortable using docker, docker compose, building a REST API, RabbitMQ.\n\nIn one project I've used APScheduler to run periodic background jobs from my REST API, but in that I pre-define the execution frequency in the code at run time, not via some configuration in a database dynamically (I think). But maybe there are similar solutions?", 'author_fullname': 't2_9d1jjuxh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to run scheduled jobs from a GUI application?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1dzyrb0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720625653.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Not sure if this is the best place to ask, but I&#39;m more of a data scientist/engineer than a fullstack developer, but maybe you guys can help.</p>\n\n<p>I have a task to create a rather basic GUI application which should be able to run on a set schedule defined from the GUI, e.g. every 30 min or every hour between 8 am and 8 pm or smth. The user should be able to change the configuration and the job should react accordingly. The Jobs would be fetching data from APIs, hopefully this makes it at least DE adjacent :).</p>\n\n<p>How would you approach this? Any references or best practices would be much appreciated.</p>\n\n<p>In principle I could code inside the application a loop that is checking if the condition is met and initiate the API calls.</p>\n\n<p>I&#39;m also wondering if this would be an appropriate use of e.g. airflow (funny enough never used it in a production setting) or something like RabbitMQ? Or is it overkill/over-engineering?</p>\n\n<p>I&#39;m comfortable using docker, docker compose, building a REST API, RabbitMQ.</p>\n\n<p>In one project I&#39;ve used APScheduler to run periodic background jobs from my REST API, but in that I pre-define the execution frequency in the code at run time, not via some configuration in a database dynamically (I think). But maybe there are similar solutions?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1dzyrb0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='boggle_thy_mind'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1dzyrb0/best_way_to_run_scheduled_jobs_from_a_gui/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1dzyrb0/best_way_to_run_scheduled_jobs_from_a_gui/', 'subreddit_subscribers': 195974, 'created_utc': 1720625653.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.890+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'A colleague of mine told me that she wants to move from working in data engineering within manufacturing to healthcare. Her big reason for this big costs and demands made on data engineers. I did caution her that in my experience, the same applies to healthcare relative to what someone does.\n\nFor those of you who have transitioned from a non-health care data engineering role to a healthcare data engineering role, how were you able to leverage your existing data engineering experience to be a fit in healthcare?\n\nI have made several transitions, so I will make a suggestion in the comments as well.', 'author_fullname': 't2_73x0n2l8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pivoting From DE Manufacturing to DE Healthcare', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0lt01', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720694248.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>A colleague of mine told me that she wants to move from working in data engineering within manufacturing to healthcare. Her big reason for this big costs and demands made on data engineers. I did caution her that in my experience, the same applies to healthcare relative to what someone does.</p>\n\n<p>For those of you who have transitioned from a non-health care data engineering role to a healthcare data engineering role, how were you able to leverage your existing data engineering experience to be a fit in healthcare?</p>\n\n<p>I have made several transitions, so I will make a suggestion in the comments as well.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e0lt01', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sqlinsix'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0lt01/pivoting_from_de_manufacturing_to_de_healthcare/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0lt01/pivoting_from_de_manufacturing_to_de_healthcare/', 'subreddit_subscribers': 195974, 'created_utc': 1720694248.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.890+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello, I am new to messaging queues and I have been struggling for a couple of hours to understand what is the best solution for this problem, I need to be able to send messages to a queue in three different ways: from one to one, from one to a specific group and from one to all. I've been experimenting with redis, kafka, rabbitmq and I can't quite figure out what the correct option is. At the moment I think that redis streams together with its pipline functionality is the simplest option, although it requires having the ids of the streams stored in groups in some way, so I am not sure if it is the most efficient solution. What other options do you think I can use?: This would be the functionality of redis: [https://github.com/redis/ioredis?tab=readme-ov-file#pipelining](https://github.com/redis/ioredis?tab=readme-ov-file#pipelining)", 'author_fullname': 't2_fbkxz1er', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best solution for one to one and one to many messaging queues?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e03bpn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720636651.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, I am new to messaging queues and I have been struggling for a couple of hours to understand what is the best solution for this problem, I need to be able to send messages to a queue in three different ways: from one to one, from one to a specific group and from one to all. I&#39;ve been experimenting with redis, kafka, rabbitmq and I can&#39;t quite figure out what the correct option is. At the moment I think that redis streams together with its pipline functionality is the simplest option, although it requires having the ids of the streams stored in groups in some way, so I am not sure if it is the most efficient solution. What other options do you think I can use?: This would be the functionality of redis: <a href="https://github.com/redis/ioredis?tab=readme-ov-file#pipelining">https://github.com/redis/ioredis?tab=readme-ov-file#pipelining</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?auto=webp&s=0706466c6ed082ebc4f029a80c0fc377a3fc2904', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=108&crop=smart&auto=webp&s=49479f63ad29797aad7cd64332c02d21a8355c94', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=216&crop=smart&auto=webp&s=e9b0252467611167eb6411973490b00160a619dd', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=320&crop=smart&auto=webp&s=b76c20a25e98770c9b4c38faf69bf41ec90e0229', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=640&crop=smart&auto=webp&s=6ba5a0fef2385697a0dc35dab717a79aa221eace', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=960&crop=smart&auto=webp&s=634c65e3b7d84e20245e3cdc5ad332dba55f25bd', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/KXwHWcfWKSrTszVKEygJ9lXuBWiO7qy3GrVRml37q5g.jpg?width=1080&crop=smart&auto=webp&s=b7859c24130ad5ea0bd72e44248b3e437e2d7805', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Rr_j3r12GE1qG6p4pxODObqEMKeRybbwj4WlpHEsmCM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e03bpn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Able_Ad9893'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e03bpn/best_solution_for_one_to_one_and_one_to_many/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e03bpn/best_solution_for_one_to_one_and_one_to_many/', 'subreddit_subscribers': 195974, 'created_utc': 1720636651.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.891+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My company has recently brought in Databricks and I am skilling myself up by making some simple jobs. The one I am using is to pull data from our ticket system via their APIs and storing into managed tables (delta format).\n\nThe process works and runs well enough but I want to look about getting some improved performance. My background is the SQL Server stack so I know the tools needed to diagnose performance there but I feel like not much of that carries over.\n\nThe way the vendors REST API structure works does limit me a bit. I have implemented some basic timing reporting on my notebooks. To pull all tickets in the system (approx. 100K) takes about an hour since I can only get 100 tickets per page and annoyingly I don't know the total number of pages. Most of the time seems to be spent in the function that writes the results to a table via a MERGE statement. I have used AI generated code to try and implement parallel processing but I need to do an A/B run vs serial processing.\n\nI want to improve my code before I throw hardware at it but I'm honestly a bit lost on how to improve this, assuming that I can. Any advise, resources on performance tuning would be greatly apricated.", 'author_fullname': 't2_a2tfz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[Databricks] First time trying to optimise a Python DB job', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0l2u8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1720692539.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720691454.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My company has recently brought in Databricks and I am skilling myself up by making some simple jobs. The one I am using is to pull data from our ticket system via their APIs and storing into managed tables (delta format).</p>\n\n<p>The process works and runs well enough but I want to look about getting some improved performance. My background is the SQL Server stack so I know the tools needed to diagnose performance there but I feel like not much of that carries over.</p>\n\n<p>The way the vendors REST API structure works does limit me a bit. I have implemented some basic timing reporting on my notebooks. To pull all tickets in the system (approx. 100K) takes about an hour since I can only get 100 tickets per page and annoyingly I don&#39;t know the total number of pages. Most of the time seems to be spent in the function that writes the results to a table via a MERGE statement. I have used AI generated code to try and implement parallel processing but I need to do an A/B run vs serial processing.</p>\n\n<p>I want to improve my code before I throw hardware at it but I&#39;m honestly a bit lost on how to improve this, assuming that I can. Any advise, resources on performance tuning would be greatly apricated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0l2u8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dsvella'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0l2u8/databricks_first_time_trying_to_optimise_a_python/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0l2u8/databricks_first_time_trying_to_optimise_a_python/', 'subreddit_subscribers': 195974, 'created_utc': 1720691454.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.891+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'We have a ODP CDC ADF pipeline that reads data from SAP whenever triggered. The table names are provided in a text file.   \nSince the dataflow uses the integration runtime cluster and it very strangely this cluster is not shared by the data flow jobs, the jobs take over 1hour for about 250 tables.  \nI wonder how could you even bring down the time of an ADF pipeline to lets say under 10-15 minutes for about 250 SAP tables.\n\nI think this cannot be done unless the cdc output is written to some messaging queue and read from there via streaming.', 'author_fullname': 't2_nm8sockru', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Can you achieve near real time processing with ADF?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0nflu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720699885.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We have a ODP CDC ADF pipeline that reads data from SAP whenever triggered. The table names are provided in a text file.<br/>\nSince the dataflow uses the integration runtime cluster and it very strangely this cluster is not shared by the data flow jobs, the jobs take over 1hour for about 250 tables.<br/>\nI wonder how could you even bring down the time of an ADF pipeline to lets say under 10-15 minutes for about 250 SAP tables.</p>\n\n<p>I think this cannot be done unless the cdc output is written to some messaging queue and read from there via streaming.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0nflu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='PutridAd9446'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0nflu/can_you_achieve_near_real_time_processing_with_adf/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0nflu/can_you_achieve_near_real_time_processing_with_adf/', 'subreddit_subscribers': 195974, 'created_utc': 1720699885.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.892+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'The high level flow is as follows:\n\nOracle -> AWS DMS -> Kinesis Data Stream -> AWS Managed Service for Apache Flink (MSF) (formerly Kinesis Data Analytics) -> apply transformation on each db transaction in the Kinesis shard using MSF -> write out result either to S3 (using Glue Data Catalog) and/or pass the result downstream to a difference Kinesis Data Stream.\n\nAnd I want this to be in near real time or as close as possible.\xa0 There will be a large volume of transactions continually pumped through from the Oracle source on all of these tables.\n\nNote - there is a single DMS (replication) CDC Task setup which captures DB transactions from about 10 different Oracle tables).\n\nI’m pretty new to Apache Flink (inc.\xa0 AWS instantiation of it - MSF).\xa0 \xa0\n\nI can see various APIs (Table/SQL etc) that can be used to perform transformation on the fly.\n\nWhat I want to achieve is to process each DB transaction on the fly in MSF (that comes in from Kinesis Data Stream from DMS), to then apply applicable joins across the different tables, which uses the “latest” set of records available at the processing point in time, and write the results out to S3.\n\nWhat I’m trying to get my head around, is how do I ensure that the join can happen against the “latest” set of records for these tables at processing time.\n\nI know that other data sources can be introduced here, but I don’t want to go back to the Oracle source, as that makes no sense and applies load to the Oracle database.\n\nAny ideas / pointers please?\n\nSecondary question, I see the Dynamic Tables and continuous query concepts supported by Flink, but where is the data for these tables stored - always in Memory?\n\nThird question - The 10 tables will already be populated with many records before this stream processing logic is applied.\xa0 I presume that I need to populate the initial transformed datasets from the 10 tables prior to turning on the CDC stream.\xa0 And I presume that the Glue Data Catalog is the logical place to store the initial full load of data?\n\nFinal question - is a Glue ETL streaming job more suited to the requirement here?', 'author_fullname': 't2_4j9ibjrg', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Processing Oracle CDC changes from redo logs using AWS DMS and Managed Service for Apache Flink (formerly Kinesis Data Analytics) in near real time', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0ahlr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720654511.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>The high level flow is as follows:</p>\n\n<p>Oracle -&gt; AWS DMS -&gt; Kinesis Data Stream -&gt; AWS Managed Service for Apache Flink (MSF) (formerly Kinesis Data Analytics) -&gt; apply transformation on each db transaction in the Kinesis shard using MSF -&gt; write out result either to S3 (using Glue Data Catalog) and/or pass the result downstream to a difference Kinesis Data Stream.</p>\n\n<p>And I want this to be in near real time or as close as possible.\xa0 There will be a large volume of transactions continually pumped through from the Oracle source on all of these tables.</p>\n\n<p>Note - there is a single DMS (replication) CDC Task setup which captures DB transactions from about 10 different Oracle tables).</p>\n\n<p>I’m pretty new to Apache Flink (inc.\xa0 AWS instantiation of it - MSF).\xa0 \xa0</p>\n\n<p>I can see various APIs (Table/SQL etc) that can be used to perform transformation on the fly.</p>\n\n<p>What I want to achieve is to process each DB transaction on the fly in MSF (that comes in from Kinesis Data Stream from DMS), to then apply applicable joins across the different tables, which uses the “latest” set of records available at the processing point in time, and write the results out to S3.</p>\n\n<p>What I’m trying to get my head around, is how do I ensure that the join can happen against the “latest” set of records for these tables at processing time.</p>\n\n<p>I know that other data sources can be introduced here, but I don’t want to go back to the Oracle source, as that makes no sense and applies load to the Oracle database.</p>\n\n<p>Any ideas / pointers please?</p>\n\n<p>Secondary question, I see the Dynamic Tables and continuous query concepts supported by Flink, but where is the data for these tables stored - always in Memory?</p>\n\n<p>Third question - The 10 tables will already be populated with many records before this stream processing logic is applied.\xa0 I presume that I need to populate the initial transformed datasets from the 10 tables prior to turning on the CDC stream.\xa0 And I presume that the Glue Data Catalog is the logical place to store the initial full load of data?</p>\n\n<p>Final question - is a Glue ETL streaming job more suited to the requirement here?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0ahlr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Calm-Conclusion9358'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0ahlr/processing_oracle_cdc_changes_from_redo_logs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0ahlr/processing_oracle_cdc_changes_from_redo_logs/', 'subreddit_subscribers': 195974, 'created_utc': 1720654511.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.892+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am looking for a tool/practical techniques that would help in maintenance of constantly updated zip files (+100 MB each). \nThey are going to be inputs for ingestion pipeline. \nData provider would share with me full zips or just the deltas between previous versions. \n\nZipping and unzipping it constantly is a nightmare. Do you know any tools/techniques that can make the process easier ?\nThank You ', 'author_fullname': 't2_9ndqvodu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Versioning Tools for business ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e091k9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720650716.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am looking for a tool/practical techniques that would help in maintenance of constantly updated zip files (+100 MB each). \nThey are going to be inputs for ingestion pipeline. \nData provider would share with me full zips or just the deltas between previous versions. </p>\n\n<p>Zipping and unzipping it constantly is a nightmare. Do you know any tools/techniques that can make the process easier ?\nThank You </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e091k9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ProfessionalPin7939'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e091k9/data_versioning_tools_for_business/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e091k9/data_versioning_tools_for_business/', 'subreddit_subscribers': 195974, 'created_utc': 1720650716.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.892+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I prefer blogs/articles over videos for learning and low effort ChatGPT trash has ruined this for me. \n\nAny good learning resources and also tips to optimize my search to find high quality blogs/articles/static sites?', 'author_fullname': 't2_g4v8h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Where do you find high-quality long-form content?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1dzze2j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720627221.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I prefer blogs/articles over videos for learning and low effort ChatGPT trash has ruined this for me. </p>\n\n<p>Any good learning resources and also tips to optimize my search to find high quality blogs/articles/static sites?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1dzze2j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='swapripper'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1dzze2j/where_do_you_find_highquality_longform_content/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1dzze2j/where_do_you_find_highquality_longform_content/', 'subreddit_subscribers': 195974, 'created_utc': 1720627221.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.893+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm part of a team that heavily relies on PySpark and SQL to work with our on-premises databases. Our workflow involves creating new columns and tables as part of various requirements. However, we've been facing a challenge with tracking the data lineage of these new attributes and tables\n\nWe need a robust data lineage solution that can track changes by parsing code ideally. How can we implement this?\n\nAll our data is on-premises, so we can't use cloud-based solutions right now. However, we are planning on moving to the cloud in the future. How will this transition affect our data lineage solution, and what should we consider to ensure a smooth migration?", 'author_fullname': 't2_vj7vfpru2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Advice on Building an On-Premises Data Lineage Solution for PySpark and SQL Workflows', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e0p5pu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720705063.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m part of a team that heavily relies on PySpark and SQL to work with our on-premises databases. Our workflow involves creating new columns and tables as part of various requirements. However, we&#39;ve been facing a challenge with tracking the data lineage of these new attributes and tables</p>\n\n<p>We need a robust data lineage solution that can track changes by parsing code ideally. How can we implement this?</p>\n\n<p>All our data is on-premises, so we can&#39;t use cloud-based solutions right now. However, we are planning on moving to the cloud in the future. How will this transition affect our data lineage solution, and what should we consider to ensure a smooth migration?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0p5pu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='throwaway_6942021253'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0p5pu/seeking_advice_on_building_an_onpremises_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0p5pu/seeking_advice_on_building_an_onpremises_data/', 'subreddit_subscribers': 195974, 'created_utc': 1720705063.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.893+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've been following GlareDB since I saw their video on building a dbt pipeline that uses different database backends. I just saw this video (https://www.youtube.com/watch?v=7W9Y\\_zZEENg), on archiving Postgres data to Delta tables. Our Postgres tables have been growing pretty quickly, and so a colleague has asked me about something like this. With all of the stuff lately about Iceberg, Delta Lake, etc. does this feel like a reasonable approach?\n\n  \n", 'author_fullname': 't2_8fuqakz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Archiving Postgres data to Delta Lake tables', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0n42s', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720698901.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve been following GlareDB since I saw their video on building a dbt pipeline that uses different database backends. I just saw this video (<a href="https://www.youtube.com/watch?v=7W9Y%5C_zZEENg">https://www.youtube.com/watch?v=7W9Y\\_zZEENg</a>), on archiving Postgres data to Delta tables. Our Postgres tables have been growing pretty quickly, and so a colleague has asked me about something like this. With all of the stuff lately about Iceberg, Delta Lake, etc. does this feel like a reasonable approach?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0n42s', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='financequestioner1'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0n42s/archiving_***_data_to_delta_lake_tables/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0n42s/archiving_***_data_to_delta_lake_tables/', 'subreddit_subscribers': 195974, 'created_utc': 1720698901.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.893+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Could you share some open-source data engineering projects that have the potential to grow? Whether it's ETL pipelines, data warehouses, real-time processing, or big data frameworks, your recommendations will be greatly appreciated!\n\n\n\nKnown languages:\n\n- C\n\n- Python\n\n- JavaScript/TypeScript\n\n- SQL\n\nP.S: I could learn Rust if needed.", 'author_fullname': 't2_yf843', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for Examples of Open Source Data Engineering Projects to contribute?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0mtf9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720697901.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Could you share some open-source data engineering projects that have the potential to grow? Whether it&#39;s ETL pipelines, data warehouses, real-time processing, or big data frameworks, your recommendations will be greatly appreciated!</p>\n\n<p>Known languages:</p>\n\n<ul>\n<li><p>C</p></li>\n<li><p>Python</p></li>\n<li><p>JavaScript/TypeScript</p></li>\n<li><p>SQL</p></li>\n</ul>\n\n<p>P.S: I could learn Rust if needed.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1e0mtf9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lukkar'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0mtf9/looking_for_examples_of_open_source_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0mtf9/looking_for_examples_of_open_source_data/', 'subreddit_subscribers': 195974, 'created_utc': 1720697901.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.894+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_bvkm0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How Canva collects 25 billion events per day', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0kogm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/Uy6XxMopNWXQ7R1740697K5p4Yi7FDs1fw1oLOlZDG8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1720689800.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'canva.dev', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.canva.dev/blog/engineering/product-analytics-event-collection/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?auto=webp&s=bf24a144724b7c55016b7a96a98fce31af978e27', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=108&crop=smart&auto=webp&s=1e2ee3cf06e210aabf509ff84b415d2316565be9', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=216&crop=smart&auto=webp&s=5397bf46f87d20c337b83d16da88cfbec719c264', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=320&crop=smart&auto=webp&s=18b4703776867743c8e712a7254b79ccca0b77b1', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=640&crop=smart&auto=webp&s=a9c230c7cddd834c8beb4c483906faea120a624a', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=960&crop=smart&auto=webp&s=b219a7e5375d7683b1100fc32a42771da78c7331', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/cBodD0X636QCwvyRnrdbXUGxQwolRn7g2kdVIHRizdw.jpg?width=1080&crop=smart&auto=webp&s=8b01966be6d215d86ee5d50cc3a458761872a891', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'je779YtRqwYd60ZV28HEV4RgmXzQQ_CLYiviEcWQ_0k'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e0kogm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rmoff'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0kogm/how_canva_collects_25_billion_events_per_day/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.canva.dev/blog/engineering/product-analytics-event-collection/', 'subreddit_subscribers': 195974, 'created_utc': 1720689800.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.894+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys!\n\nI have a task and I would much appreciate the input of experts. I get every week N number of CSV files inside a zip file in S3, each one of them between 20GB and 25GB of data. Currently I'm using pandas and an Airflow DAG to ingest them, streaming each CSV by chunks, and using dynamic tasks in Airflow to ingest all of them at the same time. However its very slow, as some of you probably  already thought, and also it crashes depending on the number of files, running out of memory.\n\nI've tried upping and lowering the number of rows to read depending on the number of files that I receive so it doesn't run out of memory but it just very slow, not even ending in one day sometimes.\n\nI think I'm too deep into the Airflow/pandas solution because at this point I don't see an alternative, probably due to lack of experience, of course.\n\nSo here I am, wondering what options would you guys use for this use case to make it faster and more robust than what I have. Is it really impossible to run in Airflow with pandas? Should I switch pandas o polars/duckdb or something like that? Maybe this is where distributed computing comes in?\n\nMuch appreciate all the answers! Let me know if you need more details abouts something. Thanks.\n\nEDIT: forgot to mention I am using multiprocessing and parallelization in Python for each file.\n\nEDIT2: forgot to mention the purpose of the task. I am reading the CSV file and transforming it into a gziped json file to reupload it to S3. After that other tasks take care of the file and upload it to the different destinations.", 'author_fullname': 't2_8zx4c5gsh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '\nStreaming CSV files from S3 - what tools to use? (currently using Airflow and Pandas)\n', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0jzsx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1720689140.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720686885.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys!</p>\n\n<p>I have a task and I would much appreciate the input of experts. I get every week N number of CSV files inside a zip file in S3, each one of them between 20GB and 25GB of data. Currently I&#39;m using pandas and an Airflow DAG to ingest them, streaming each CSV by chunks, and using dynamic tasks in Airflow to ingest all of them at the same time. However its very slow, as some of you probably  already thought, and also it crashes depending on the number of files, running out of memory.</p>\n\n<p>I&#39;ve tried upping and lowering the number of rows to read depending on the number of files that I receive so it doesn&#39;t run out of memory but it just very slow, not even ending in one day sometimes.</p>\n\n<p>I think I&#39;m too deep into the Airflow/pandas solution because at this point I don&#39;t see an alternative, probably due to lack of experience, of course.</p>\n\n<p>So here I am, wondering what options would you guys use for this use case to make it faster and more robust than what I have. Is it really impossible to run in Airflow with pandas? Should I switch pandas o polars/duckdb or something like that? Maybe this is where distributed computing comes in?</p>\n\n<p>Much appreciate all the answers! Let me know if you need more details abouts something. Thanks.</p>\n\n<p>EDIT: forgot to mention I am using multiprocessing and parallelization in Python for each file.</p>\n\n<p>EDIT2: forgot to mention the purpose of the task. I am reading the CSV file and transforming it into a gziped json file to reupload it to S3. After that other tasks take care of the file and upload it to the different destinations.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0jzsx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Sumdon_123'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0jzsx/streaming_csv_files_from_s3_what_tools_to_use/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0jzsx/streaming_csv_files_from_s3_what_tools_to_use/', 'subreddit_subscribers': 195974, 'created_utc': 1720686885.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "It's generally recommended that Avro is the preferred format for ingesting into Kafka. Why?\n\nHow does Kafka leverage the capabilities of Avro? For Kafka, it's just bytes in the end. I can imagine it's usefulness if Kafka's internal storage stored the incoming records as big blobs of Avro's\n\nTaking Parquet as an example, batch jobs pack several rows into a single parquet file.\n\nMaybe it's my experience with the systems I've worked with where individual rows are emitted to kafka as separate messages. \n\nDo you agree that it need not be this way? Producers can batch together multiple records into 1 single Avro and publish them to Kafka as a single message?", 'author_fullname': 't2_hmsc4tzi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why Avro for Kafka', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e0r1ei', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720710056.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>It&#39;s generally recommended that Avro is the preferred format for ingesting into Kafka. Why?</p>\n\n<p>How does Kafka leverage the capabilities of Avro? For Kafka, it&#39;s just bytes in the end. I can imagine it&#39;s usefulness if Kafka&#39;s internal storage stored the incoming records as big blobs of Avro&#39;s</p>\n\n<p>Taking Parquet as an example, batch jobs pack several rows into a single parquet file.</p>\n\n<p>Maybe it&#39;s my experience with the systems I&#39;ve worked with where individual rows are emitted to kafka as separate messages. </p>\n\n<p>Do you agree that it need not be this way? Producers can batch together multiple records into 1 single Avro and publish them to Kafka as a single message?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e0r1ei', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No_Direction_5276'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0r1ei/why_avro_for_kafka/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0r1ei/why_avro_for_kafka/', 'subreddit_subscribers': 195974, 'created_utc': 1720710056.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all, \n\nI'm currently working on a project where I need to process multiple XML files (around 100 and a few GBs of data each week) using PySpark. However the XML files do not have consistent schemas. \n\nHere's a bit more detail about my situation: \n\nThe XML files are stored in a single directory. The files vary in structure, with different tags and nested elements. I need to extract meaningful data from all the files and consolidate it into a unified DataFrame for further analysis. \n\nI'm looking for best practices and advice on how to approach this problem. Specifically: \n\n**Schema Inference**: How can I efficiently infer schemas for XML files with different structures? Are there any tools or libraries that can help with this in a scalable way? \n\n**Handling Schema Variations**: What are the best strategies for handling files with varying schemas? Should I preprocess the files to standardize them, or is there a more dynamic approach? \n\n**Performance Optimization**: Given the potential size and complexity of the XML files, what performance considerations should I keep in mind? Are there any techniques to optimize the reading and processing of these files in PySpark?\n\nSo far my plan is: \n\n* To read one file at a time. \n* Load the data into a Data Frame. \n* Save the schema (sort of master schema). \n* Cycle through each file and compare both schemas and modify the master schema until there is a irreconcilable difference then I don't process that file and add that file name into an error log.\n\nIs this a naive approach? Are there documentations/repos/books I should be reading to propose a robust solution for this situation?\n\nThank you in advance.", 'author_fullname': 't2_erjq3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Advice on Handling Multiple XML Files with Different Schemas in PySpark', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e0pbic', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720705536.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, </p>\n\n<p>I&#39;m currently working on a project where I need to process multiple XML files (around 100 and a few GBs of data each week) using PySpark. However the XML files do not have consistent schemas. </p>\n\n<p>Here&#39;s a bit more detail about my situation: </p>\n\n<p>The XML files are stored in a single directory. The files vary in structure, with different tags and nested elements. I need to extract meaningful data from all the files and consolidate it into a unified DataFrame for further analysis. </p>\n\n<p>I&#39;m looking for best practices and advice on how to approach this problem. Specifically: </p>\n\n<p><strong>Schema Inference</strong>: How can I efficiently infer schemas for XML files with different structures? Are there any tools or libraries that can help with this in a scalable way? </p>\n\n<p><strong>Handling Schema Variations</strong>: What are the best strategies for handling files with varying schemas? Should I preprocess the files to standardize them, or is there a more dynamic approach? </p>\n\n<p><strong>Performance Optimization</strong>: Given the potential size and complexity of the XML files, what performance considerations should I keep in mind? Are there any techniques to optimize the reading and processing of these files in PySpark?</p>\n\n<p>So far my plan is: </p>\n\n<ul>\n<li>To read one file at a time. </li>\n<li>Load the data into a Data Frame. </li>\n<li>Save the schema (sort of master schema). </li>\n<li>Cycle through each file and compare both schemas and modify the master schema until there is a irreconcilable difference then I don&#39;t process that file and add that file name into an error log.</li>\n</ul>\n\n<p>Is this a naive approach? Are there documentations/repos/books I should be reading to propose a robust solution for this situation?</p>\n\n<p>Thank you in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0pbic', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SirAlejo'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0pbic/seeking_advice_on_handling_multiple_xml_files/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0pbic/seeking_advice_on_handling_multiple_xml_files/', 'subreddit_subscribers': 195974, 'created_utc': 1720705536.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone! \n\nCurrently at a mid sized tech company working a pretty chill schedule and planning on starting to do some contracting and smaller jobs to get more income. Plan is to eventually quit my FT job and stick with contracting or consulting or w/e you want go call it.\n\nIve gotten a few linkedin dms from people looking to hire for year long gigs or project length wise. My question is how different from a normal job this is? Will I get asked to solve LC questions? If I end up working there, am I supposed to be “part of the family” BS of corporate world? Anything else I should know?', 'author_fullname': 't2_4s2dogl9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data contracting in US', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e0p5my', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720705055.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone! </p>\n\n<p>Currently at a mid sized tech company working a pretty chill schedule and planning on starting to do some contracting and smaller jobs to get more income. Plan is to eventually quit my FT job and stick with contracting or consulting or w/e you want go call it.</p>\n\n<p>Ive gotten a few linkedin dms from people looking to hire for year long gigs or project length wise. My question is how different from a normal job this is? Will I get asked to solve LC questions? If I end up working there, am I supposed to be “part of the family” BS of corporate world? Anything else I should know?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e0p5my', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rudboi12'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0p5my/data_contracting_in_us/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0p5my/data_contracting_in_us/', 'subreddit_subscribers': 195974, 'created_utc': 1720705055.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.896+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Is there anybody here who can explain the pricing difference between the following two pages from prefect? Are those offerings different?\n\n[https://next.prefect.io/pricing](https://next.prefect.io/pricing) (pro: $450/month)\n\n[https://www.prefect.io/pricing](https://www.prefect.io/pricing) (pro:$1850/month)', 'author_fullname': 't2_9knk666s', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'prefect pricing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0lbjv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720692399.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Is there anybody here who can explain the pricing difference between the following two pages from prefect? Are those offerings different?</p>\n\n<p><a href="https://next.prefect.io/pricing">https://next.prefect.io/pricing</a> (pro: $450/month)</p>\n\n<p><a href="https://www.prefect.io/pricing">https://www.prefect.io/pricing</a> (pro:$1850/month)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?auto=webp&s=6d4b1a4ef7d5737dc31e42292a5465c7048e03e0', 'width': 1120, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=108&crop=smart&auto=webp&s=cef2f0c48cd28652642ac24c6e96a209a37db8ac', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=216&crop=smart&auto=webp&s=68f2fcde9891db21076bd56e462ef08039b9da03', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=320&crop=smart&auto=webp&s=1d6ac329592d0920c929e6ac5be5acedd7c93314', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=640&crop=smart&auto=webp&s=1c7e5f83c9654db5fb4f9bdaef4b37d782cf84c8', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=960&crop=smart&auto=webp&s=e9893f647ace5163f18955b4981ecac7b8091b34', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/gOY2K3nL_A_MUTka-wx0J2ML1KsjLYACn5xIQf0SLKw.jpg?width=1080&crop=smart&auto=webp&s=b730d517b994c8eaa88523f2d5b5fecef181b3a6', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'KKrk7Yz4oUrmfBr8AtMnzOKLkq1m01QBCzaMihkvjIA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0lbjv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='limartje'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0lbjv/prefect_pricing/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0lbjv/prefect_pricing/', 'subreddit_subscribers': 195974, 'created_utc': 1720692399.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.896+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_gxesw7ji', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bytebase 2.21.0 released, Schema change and version control for PostgreSQL/MySQL/Snowflake ...', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0l483', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'https://b.thumbs.redditmedia.com/y-WjHlkBPLwFBGzQ5IMjE59MXOT2qm4NhfaEVq7mRqE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1720691609.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'bytebase.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.bytebase.com/changelog/bytebase-2-21-0/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?auto=webp&s=fb4bb40afff73cf9122b98579c87ce0527fa0701', 'width': 1600, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=108&crop=smart&auto=webp&s=6b4ebdde831fbd56f8d03305368ae78dec83c308', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=216&crop=smart&auto=webp&s=48fca7d28ec4858717c6006d4e852721b2ced23e', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=320&crop=smart&auto=webp&s=82dbb674e3f18a59caa3636c92d88e99b53cf376', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=640&crop=smart&auto=webp&s=2947b6066992e8b72a507a9246ca96e0d457b53b', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=960&crop=smart&auto=webp&s=ae741a735c6584374d0c58ad3c44fd695ad13c59', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/D5j9Bd93cPwqjpcSxcem_rXCtLLJsW-QVSgseYB4RXM.jpg?width=1080&crop=smart&auto=webp&s=1b0cc09a4dc2f7bea6242c4a90787422f5accf0a', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'F9uDoslXE8iA-8FQ_34yc2VGvDA-Axc0S7EJtmdhdRk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e0l483', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Adela_freedom'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0l483/bytebase_2210_released_schema_change_and_version/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.bytebase.com/changelog/bytebase-2-21-0/', 'subreddit_subscribers': 195974, 'created_utc': 1720691609.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.896+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\nI’m starting my first fully fledged DE position in a couple of months, transferring from a SQL analyst position that was about 20% DE and on-prem technology. I have 15+ years of SQL skills and about 1 year’s experience with Python.\n\nThe DE position I’m moving to predominately uses SQL, a little bit of python (but scope to increase this), and the tech stack they use is Azure data factory, Databricks and datalake.\n\nIn my two months before I start I wish to personally work on my skillset and understanding to help ensure I can at least ‘hit the ground walking’ when I first start.\n\nBased on your experience, if you were in my shoes:\n- What questions would you ask your soon to be manager to ascertain what you could be working on in the interim?\n- What general things, based on the tech stack above would you be learning about/working on before starting?\n\nAny advice or thoughts would be much appreciated.\n', 'author_fullname': 't2_aghkfqir', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Starting new position - Advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0k7w5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720687854.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>I’m starting my first fully fledged DE position in a couple of months, transferring from a SQL analyst position that was about 20% DE and on-prem technology. I have 15+ years of SQL skills and about 1 year’s experience with Python.</p>\n\n<p>The DE position I’m moving to predominately uses SQL, a little bit of python (but scope to increase this), and the tech stack they use is Azure data factory, Databricks and datalake.</p>\n\n<p>In my two months before I start I wish to personally work on my skillset and understanding to help ensure I can at least ‘hit the ground walking’ when I first start.</p>\n\n<p>Based on your experience, if you were in my shoes:\n- What questions would you ask your soon to be manager to ascertain what you could be working on in the interim?\n- What general things, based on the tech stack above would you be learning about/working on before starting?</p>\n\n<p>Any advice or thoughts would be much appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e0k7w5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='metis84'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0k7w5/starting_new_position_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0k7w5/starting_new_position_advice/', 'subreddit_subscribers': 195974, 'created_utc': 1720687854.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.897+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all,\n\nI work in DA/DM/BI with 5 YOE and started looking for opportunities to climb up, perhaps as a Single Contributor, but mainly focused on lead or manager roles. Many openings mention something along the lines of ELT/ETL experience, modeling, database/warehouse, architecture, engineering. Not in an unicorn way, but that the candidate is aware what this is and how it works on high level. For example, they might not know how to program in Python, but very well knows what Python can do in this entire data chain.\n\nFirst of all, I get it. In a leadership position you want someone who is coming from at least one specialization/subdomain, with related knowledge or better yet, experience of things from the other subdomains. This is exactly what I like to achieve in my spare time.\n\nSince I'm coming from the 'business-leaning-side', I like to develop the engineering skills to become a better candidate for leadership roles. I won't be able to do this at my current employer due to several reasons, and need to do this at home.\n\nWhat do I like to build and learn?\n\n- a full pipeline from raw data at the start, to several tables of cleaned data, each table for a different purpose for example BI reporting  \n- transformation steps in between, varying from data wrangling, splitting up tables, merging tables ... let's go crazy so I have a nice challenge to figure out stuff (aka, I don't want to have a single line, that might be too easy and it won't impress anyone)  \n- if possible based on the available data, I like to enforce a Current State that is just messy as hell (since that's how most companies are it seems), so I can change it into a Future State where the architecture and engineering is as close to best practices as possible (so I can share this as an improvement story at interviews)\n\nMy questions to you are, what do you recommend I pick in terms of platforms, apps, technology? This is going to sounds horrible, but I like to spend as little money as possible, or perhaps even none.\n\nThe following are already known to me:\n\n- Microsoft 365 E5 Developers Account (I have this, not sure if my Fabric trial is still active)  \n- DataBricks Community Edition (this has no time limit, but not entirely sure if I can build pipelines and connect to external sources/apps)  \n- Excel, Power BI Desktop (have these at home)  \n- Supabase (they have a free tier without time limit, it's PostgreSQL I believe)  \n- Docker (no-go, I tried to understand this so many times, but it's rocket science to me, also means that a lot of OS stacks won't be available to me for this endeavor)\n\nThe most important thing: I want to have fun being curious.\n\nLike LEGO, bunch of blocks, put them together and hey cool, it works .. I think, why not, oh it actually does, nice! I don't want to read research papers and books first, with all do respect.\n\nLooking forward to see your recommendations.\n\nThanks.", 'author_fullname': 't2_kh3ubtce', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DE practice for a DA/BI person', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e00r0q', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720630472.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I work in DA/DM/BI with 5 YOE and started looking for opportunities to climb up, perhaps as a Single Contributor, but mainly focused on lead or manager roles. Many openings mention something along the lines of ELT/ETL experience, modeling, database/warehouse, architecture, engineering. Not in an unicorn way, but that the candidate is aware what this is and how it works on high level. For example, they might not know how to program in Python, but very well knows what Python can do in this entire data chain.</p>\n\n<p>First of all, I get it. In a leadership position you want someone who is coming from at least one specialization/subdomain, with related knowledge or better yet, experience of things from the other subdomains. This is exactly what I like to achieve in my spare time.</p>\n\n<p>Since I&#39;m coming from the &#39;business-leaning-side&#39;, I like to develop the engineering skills to become a better candidate for leadership roles. I won&#39;t be able to do this at my current employer due to several reasons, and need to do this at home.</p>\n\n<p>What do I like to build and learn?</p>\n\n<ul>\n<li>a full pipeline from raw data at the start, to several tables of cleaned data, each table for a different purpose for example BI reporting<br/></li>\n<li>transformation steps in between, varying from data wrangling, splitting up tables, merging tables ... let&#39;s go crazy so I have a nice challenge to figure out stuff (aka, I don&#39;t want to have a single line, that might be too easy and it won&#39;t impress anyone)<br/></li>\n<li>if possible based on the available data, I like to enforce a Current State that is just messy as hell (since that&#39;s how most companies are it seems), so I can change it into a Future State where the architecture and engineering is as close to best practices as possible (so I can share this as an improvement story at interviews)</li>\n</ul>\n\n<p>My questions to you are, what do you recommend I pick in terms of platforms, apps, technology? This is going to sounds horrible, but I like to spend as little money as possible, or perhaps even none.</p>\n\n<p>The following are already known to me:</p>\n\n<ul>\n<li>Microsoft 365 E5 Developers Account (I have this, not sure if my Fabric trial is still active)<br/></li>\n<li>DataBricks Community Edition (this has no time limit, but not entirely sure if I can build pipelines and connect to external sources/apps)<br/></li>\n<li>Excel, Power BI Desktop (have these at home)<br/></li>\n<li>Supabase (they have a free tier without time limit, it&#39;s PostgreSQL I believe)<br/></li>\n<li>Docker (no-go, I tried to understand this so many times, but it&#39;s rocket science to me, also means that a lot of OS stacks won&#39;t be available to me for this endeavor)</li>\n</ul>\n\n<p>The most important thing: I want to have fun being curious.</p>\n\n<p>Like LEGO, bunch of blocks, put them together and hey cool, it works .. I think, why not, oh it actually does, nice! I don&#39;t want to read research papers and books first, with all do respect.</p>\n\n<p>Looking forward to see your recommendations.</p>\n\n<p>Thanks.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e00r0q', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SquidsAndMartians'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e00r0q/de_practice_for_a_dabi_person/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e00r0q/de_practice_for_a_dabi_person/', 'subreddit_subscribers': 195974, 'created_utc': 1720630472.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.897+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7c95be80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm evaluating Workato and I know it's a great player in the SMB space, but curious what people think about it for more of an enterprise sized organization. The cost doesn't seem too bad for the initial phases but my concern is understanding how their model scales?\n\nFrom my understanding, it sounds the number of tasks and recipes would increase pretty rapidly so curious how it would increase my costs.  ", 'author_fullname': 't2_vr7vi3mtm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Workato costs? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e0ephb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720666932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m evaluating Workato and I know it&#39;s a great player in the SMB space, but curious what people think about it for more of an enterprise sized organization. The cost doesn&#39;t seem too bad for the initial phases but my concern is understanding how their model scales?</p>\n\n<p>From my understanding, it sounds the number of tasks and recipes would increase pretty rapidly so curious how it would increase my costs.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e0ephb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Much-North-7732'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e0ephb/workato_costs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e0ephb/workato_costs/', 'subreddit_subscribers': 195974, 'created_utc': 1720666932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-11T15:12:37.897+0000] {python.py:194} INFO - Done. Returned value was: None
[2024-07-11T15:12:37.905+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extract, execution_date=20240711T151236, start_date=20240711T151237, end_date=20240711T151237
[2024-07-11T15:12:37.949+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-07-11T15:12:37.959+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
